{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldatapreds = pd.read_csv(\"data/full_data.csv\")\n",
    "fulldatapreds = fulldatapreds.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_patients = glob.glob(\"/home/NETID/gramacha/data/Report_matcher/Split_Dataset2/Dev/Index/*.txt\")\n",
    "exclude_ids = [x.split(\"/\")[-1].split(\".\")[0] for x in dev_patients]\n",
    "fulldatapreds = fulldatapreds[~fulldatapreds['index_key'].isin(exclude_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldwise_results = [0,1,2,3,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression with reduced dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default threshold\n",
    "lrd_results_dir = \"/home/NETID/gramacha/projects/report_matcher/qanda/SVM/results_results_0602/logistic_reduced100_3V_nostop/\"\n",
    "\n",
    "lrd_result_files = []\n",
    "lrd_full_preds = pd.DataFrame()\n",
    "\n",
    "for fold_num in foldwise_results:\n",
    "    fold_results_file = f\"{lrd_results_dir}test/test_fold{str(fold_num)}__defpreds_predictions.csv\"\n",
    "    lrd_result_files.append(fold_results_file)\n",
    "    fold_results = pd.read_csv(fold_results_file).drop(columns=[\"Unnamed: 0\"])\n",
    "    lrd_full_preds = pd.concat([lrd_full_preds, fold_results])\n",
    "    \n",
    "lrd_full_preds[\"pred\"] = lrd_full_preds[\"pred\"].astype('int')\n",
    "\n",
    "for idx, row in fulldatapreds.iterrows():\n",
    "    fulldatapreds.loc[idx, 'LR_0.5'] = lrd_full_preds.loc[lrd_full_preds['cand_key']==row['cand_key'], \"pred\"].values[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fold wise best threshold\n",
    "lrd_result_files = []\n",
    "lrd_full_preds = pd.DataFrame()\n",
    "fold_best_t = [0.25,0.3,0.4,0.575,0.475]\n",
    "\n",
    "for fold_num in foldwise_results:\n",
    "    fold_results_file = f\"{lrd_results_dir}test/test_fold{str(fold_num)}__THRESH{str(fold_best_t[fold_num])}_predictions.csv\"\n",
    "    lrd_result_files.append(fold_results_file)\n",
    "    fold_results = pd.read_csv(fold_results_file).drop(columns=[\"Unnamed: 0\"])\n",
    "    lrd_full_preds = pd.concat([lrd_full_preds, fold_results])\n",
    "    \n",
    "lrd_full_preds[\"pred\"] = lrd_full_preds[\"pred\"].astype('int')\n",
    "\n",
    "for idx, row in fulldatapreds.iterrows():\n",
    "    fulldatapreds.loc[idx, 'LR_fold_best'] = lrd_full_preds.loc[lrd_full_preds['cand_key']==row['cand_key'], \"pred\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overall best thresholds 0.3\n",
    "lrd_result_files = []\n",
    "lrd_full_preds = pd.DataFrame()\n",
    "overall_best = 0.3\n",
    "\n",
    "for fold_num in foldwise_results:\n",
    "    fold_results_file = f\"{lrd_results_dir}test/test_fold{str(fold_num)}__THRESH{str(overall_best)}_predictions.csv\"\n",
    "    lrd_result_files.append(fold_results_file)\n",
    "    fold_results = pd.read_csv(fold_results_file).drop(columns=[\"Unnamed: 0\"])\n",
    "    lrd_full_preds = pd.concat([lrd_full_preds, fold_results])\n",
    "    \n",
    "lrd_full_preds[\"pred\"] = lrd_full_preds[\"pred\"].astype('int')\n",
    "\n",
    "for idx, row in fulldatapreds.iterrows():\n",
    "    fulldatapreds.loc[idx, 'LR_0.3_overall'] = lrd_full_preds.loc[lrd_full_preds['cand_key']==row['cand_key'], \"pred\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overall best threshold 0.4\n",
    "lrd_result_files = []\n",
    "lrd_full_preds = pd.DataFrame()\n",
    "overall_best = 0.4\n",
    "\n",
    "for fold_num in foldwise_results:\n",
    "    fold_results_file = f\"{lrd_results_dir}test/test_fold{str(fold_num)}__THRESH{str(overall_best)}_predictions.csv\"\n",
    "    lrd_result_files.append(fold_results_file)\n",
    "    fold_results = pd.read_csv(fold_results_file).drop(columns=[\"Unnamed: 0\"])\n",
    "    lrd_full_preds = pd.concat([lrd_full_preds, fold_results])\n",
    "    \n",
    "lrd_full_preds[\"pred\"] = lrd_full_preds[\"pred\"].astype('int')\n",
    "\n",
    "for idx, row in fulldatapreds.iterrows():\n",
    "    fulldatapreds.loc[idx, 'LR_0.4_overall'] = lrd_full_preds.loc[lrd_full_preds['cand_key']==row['cand_key'], \"pred\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results_dir = \"/home/NETID/gramacha/projects/report_matcher/qanda/SVM/results_results_0602/svm_reduced100_3V_nostop_seed51/\"\n",
    "\n",
    "svm_result_files = []\n",
    "svm_full_preds = pd.DataFrame()\n",
    "\n",
    "for fold_num in foldwise_results:\n",
    "    fold_results_file = f\"{svm_results_dir}/test/test_fold{str(fold_num)}__defpreds_predictions.csv\"\n",
    "    svm_result_files.append(fold_results_file)\n",
    "    fold_results = pd.read_csv(fold_results_file).drop(columns=[\"Unnamed: 0\"])\n",
    "    svm_full_preds = pd.concat([svm_full_preds, fold_results])\n",
    "    \n",
    "svm_full_preds[\"pred\"] = svm_full_preds[\"pred\"].astype('int')\n",
    "\n",
    "for idx, row in fulldatapreds.iterrows():    \n",
    "    fulldatapreds.loc[idx, 'svm_pred'] = svm_full_preds.loc[svm_full_preds['cand_key']==row['cand_key'], \"pred\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fold wise best threshold\n",
    "svm_result_files = []\n",
    "svm_full_preds = pd.DataFrame()\n",
    "fold_best_t = [0.045, 0.055, 0.085, 0.075, 0.055]\n",
    "\n",
    "for fold_num in foldwise_results:\n",
    "    fold_results_file = f\"{svm_results_dir}test/test_fold{str(fold_num)}__THRESH{str(fold_best_t[fold_num])}_predictions.csv\"\n",
    "    svm_result_files.append(fold_results_file)\n",
    "    fold_results = pd.read_csv(fold_results_file).drop(columns=[\"Unnamed: 0\"])\n",
    "    svm_full_preds = pd.concat([svm_full_preds, fold_results])\n",
    "    \n",
    "svm_full_preds[\"pred\"] = svm_full_preds[\"pred\"].astype('int')\n",
    "\n",
    "for idx, row in fulldatapreds.iterrows():\n",
    "    fulldatapreds.loc[idx, 'svm_fold_best'] = svm_full_preds.loc[svm_full_preds['cand_key']==row['cand_key'], \"pred\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overall best threshold\n",
    "svm_result_files = []\n",
    "svm_full_preds = pd.DataFrame()\n",
    "overall_best_t = 0.055\n",
    "\n",
    "for fold_num in foldwise_results:\n",
    "    fold_results_file = f\"{svm_results_dir}test/test_fold{str(fold_num)}__THRESH{str(overall_best_t)}_predictions.csv\"\n",
    "    svm_result_files.append(fold_results_file)\n",
    "    fold_results = pd.read_csv(fold_results_file).drop(columns=[\"Unnamed: 0\"])\n",
    "    svm_full_preds = pd.concat([svm_full_preds, fold_results])\n",
    "    \n",
    "svm_full_preds[\"pred\"] = svm_full_preds[\"pred\"].astype('int')\n",
    "\n",
    "for idx, row in fulldatapreds.iterrows():\n",
    "    fulldatapreds.loc[idx, 'svm_overall'] = svm_full_preds.loc[svm_full_preds['cand_key']==row['cand_key'], \"pred\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "##read other results from previously created files\n",
    "\n",
    "datapreds = pd.read_csv(\"/home/NETID/gramacha/projects/report_matcher/qanda/sig test/full_data_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in fulldatapreds.iterrows():\n",
    "    fulldatapreds.loc[idx, 'gpt4Best_pred'] = datapreds.loc[datapreds['cand_key']==row['cand_key'], \"gpt4Best_pred\"].values[0]\n",
    "    fulldatapreds.loc[idx, 'gpt4Base_pred'] = datapreds.loc[datapreds['cand_key']==row['cand_key'], \"gpt4Base_pred\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldatapreds.to_csv(\"/home/NETID/gramacha/projects/report_matcher/qanda/sig test/latestres.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['round', 'index_key', 'cand_key', 'input', 'label', 'LR_0.5',\n",
       "       'LR_fold_best', 'LR_0.3_overall', 'LR_0.4_overall', 'svm_pred',\n",
       "       'svm_overall', 'svm_fold_best', 'gpt4Best_pred', 'gpt4Base_pred'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fulldatapreds.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT Base\n",
    "\n",
    "gpt_results_dir = \"/home/NETID/gramacha/projects/report_matcher/qanda/sig test/GPT4_preds/\"\n",
    "gpt_results_file = gpt_results_dir + \"GPT4_Best_Wrong_Predictions.csv\"\n",
    "\n",
    "gpt_results = pd.read_csv(gpt_results_file)\n",
    "\n",
    "gpt_results_colname = \"gpt4Best_pred\"\n",
    "# gpt_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrate GPT predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fulldatapreds[\"gpt_preds\"] = 0\n",
    "# # fulldatapreds[\"gpt_incorrect\"] = 0\n",
    "# round12_Sample_318\n",
    "# correct label = 1\n",
    "# gpt label =  -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fulldatapreds = d.copy()\n",
    "fulldatapreds[gpt_results_colname] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round01_Sample_30\n",
      "correct label = 0\n",
      "gpt label =  1\n",
      "round04_Sample_34\n",
      "correct label = 0\n",
      "gpt label =  4\n",
      "round06_Sample_32\n",
      "correct label = 0\n",
      "gpt label =  -1\n",
      "round07_Sample_79\n",
      "correct label = 0\n",
      "gpt label =  6\n",
      "round08_Sample_113\n",
      "correct label = 0\n",
      "gpt label =  1\n",
      "round09_Sample_130\n",
      "correct label = 0\n",
      "gpt label =  1\n",
      "round09_Sample_137\n",
      "correct label = 0\n",
      "gpt label =  -1\n",
      "round09_Sample_145\n",
      "correct label = 0\n",
      "gpt label =  2\n",
      "round10_Sample_167\n",
      "correct label = 0\n",
      "gpt label =  -1\n",
      "round11_Sample_229\n",
      "correct label = 0\n",
      "gpt label =  2\n"
     ]
    }
   ],
   "source": [
    "for ikey in np.unique(gpt_results[\"Sample_ID\"]).tolist():\n",
    "    try:\n",
    "        prediction = gpt_results[gpt_results[\"Sample_ID\"]==ikey]['Pred'].values[0]\n",
    "\n",
    "        if prediction == -1:\n",
    "            continue\n",
    "\n",
    "        gt = gpt_results[gpt_results[\"Sample_ID\"]==ikey]['Gold'].values[0]\n",
    "\n",
    "        correctlabels = list(fulldatapreds[fulldatapreds[\"index_key\"]==ikey]['label'].values)\n",
    "        \n",
    "        if 1 in correctlabels:\n",
    "            label = correctlabels.index(1)\n",
    "            assert(label+1 == gt) #safety check to compare the GPT gt is correct or not\n",
    "        else:\n",
    "            label = -1\n",
    "            assert(gt == label)\n",
    "\n",
    "        candidates = fulldatapreds[fulldatapreds[\"index_key\"]==ikey]['cand_key']\n",
    "        gpt_pred_cand = list(candidates)[prediction-1]\n",
    "\n",
    "        fulldatapreds.loc[fulldatapreds[\"cand_key\"]==gpt_pred_cand, gpt_results_colname] = 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(ikey)\n",
    "        print(\"correct label =\", label+1)\n",
    "        print(\"gpt label = \", gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>index_key</th>\n",
       "      <th>cand_key</th>\n",
       "      <th>input</th>\n",
       "      <th>label</th>\n",
       "      <th>llama_pred</th>\n",
       "      <th>logisticDR_pred</th>\n",
       "      <th>svm_pred</th>\n",
       "      <th>gpt4Best_pred</th>\n",
       "      <th>gpt_correct</th>\n",
       "      <th>gpt4Base_pred</th>\n",
       "      <th>unique_keys</th>\n",
       "      <th>LongFormer_pred</th>\n",
       "      <th>logisticDR_Version2_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [round, index_key, cand_key, input, label, llama_pred, logisticDR_pred, svm_pred, gpt4Best_pred, gpt_correct, gpt4Base_pred, unique_keys, LongFormer_pred, logisticDR_Version2_pred]\n",
       "Index: []"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fulldatapreds[(fulldatapreds['label'] == 1) & (fulldatapreds[gpt_results_colname] == 1)]\n",
    "# fulldatapreds[fulldatapreds['index_key'] == \"round03_Sample_34\"]\n",
    "# fulldatapreds[fulldatapreds['index_key'] == \"round15_Sample_555\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95      5389\n",
      "           1       0.00      0.00      0.00       418\n",
      "\n",
      "    accuracy                           0.91      5807\n",
      "   macro avg       0.46      0.49      0.48      5807\n",
      "weighted avg       0.86      0.91      0.88      5807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(fulldatapreds['label'], fulldatapreds[gpt_results_colname]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctpred_idx = fulldatapreds[~fulldatapreds[\"index_key\"].isin(gpt_results[\"Sample_ID\"])]\n",
    "for idx, row in correctpred_idx.iterrows():\n",
    "    # print(fulldatapreds.loc[idx, gpt_results_colname])\n",
    "    # print(row[\"label\"])\n",
    "    # print(idx)\n",
    "    # break\n",
    "    fulldatapreds.loc[idx, gpt_results_colname] = row[\"label\"]\n",
    "    fulldatapreds.loc[idx, \"gpt_correct\"] = \"correct prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      5389\n",
      "           1       0.72      0.78      0.75       418\n",
      "\n",
      "    accuracy                           0.96      5807\n",
      "   macro avg       0.85      0.88      0.86      5807\n",
      "weighted avg       0.96      0.96      0.96      5807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(fulldatapreds['label'], fulldatapreds[gpt_results_colname]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fulldatapreds.to_csv(\"/home/NETID/gramacha/projects/report_matcher/qanda/sig test/full_data_preds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#incorrect value in GPT preds sheet\n",
    "# round12_Sample_318\n",
    "# correct label = 1\n",
    "# gpt label =  -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TP: 324 FP: 60 TN: 108 FN: 94\n",
      "P: 0.844 R: 0.775 F1: 0.808\n",
      "0.808\n"
     ]
    }
   ],
   "source": [
    "#validate if the F1 matches\n",
    "import sys\n",
    "sys.path.append(\"/home/NETID/gramacha/projects/report_matcher/qanda/\")\n",
    "import helperfactory.patientlevel_metrics as pat\n",
    "\n",
    "scores, pat_scores_msg, tpnpcounter = pat.get_patient_level_metrics(fulldatapreds, gt_colname='label', pred_colname=gpt_results_colname, \n",
    "                                                                    results_dir=gpt_results_dir, resultsfilename=gpt_results_colname)\n",
    "print(pat_scores_msg)\n",
    "print(scores[\"F1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LongFormer results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldatapreds = pd.read_csv(\"/home/NETID/gramacha/projects/report_matcher/qanda/sig test/full_data_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round</th>\n",
       "      <th>index_key</th>\n",
       "      <th>cand_key</th>\n",
       "      <th>gt</th>\n",
       "      <th>pred</th>\n",
       "      <th>prob</th>\n",
       "      <th>logit</th>\n",
       "      <th>best_pred</th>\n",
       "      <th>best_predfromprob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>round13</td>\n",
       "      <td>round13_Sample_381</td>\n",
       "      <td>round13_Sample_381_01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002097</td>\n",
       "      <td>-6.165163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>round13</td>\n",
       "      <td>round13_Sample_381</td>\n",
       "      <td>round13_Sample_381_02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>-7.533374</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>round13</td>\n",
       "      <td>round13_Sample_381</td>\n",
       "      <td>round13_Sample_381_03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>-7.591379</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>round13</td>\n",
       "      <td>round13_Sample_381</td>\n",
       "      <td>round13_Sample_381_04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>-7.568815</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>round13</td>\n",
       "      <td>round13_Sample_381</td>\n",
       "      <td>round13_Sample_381_05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>-7.631122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>round03</td>\n",
       "      <td>round03_Sample_29</td>\n",
       "      <td>round03_Sample_29_16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>-7.221320</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>round03</td>\n",
       "      <td>round03_Sample_29</td>\n",
       "      <td>round03_Sample_29_17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>-6.207634</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>round03</td>\n",
       "      <td>round03_Sample_29</td>\n",
       "      <td>round03_Sample_29_18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>-7.228040</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>round03</td>\n",
       "      <td>round03_Sample_29</td>\n",
       "      <td>round03_Sample_29_19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>-7.180627</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>round03</td>\n",
       "      <td>round03_Sample_29</td>\n",
       "      <td>round03_Sample_29_20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017674</td>\n",
       "      <td>-4.017853</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5221 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       round           index_key               cand_key  gt  pred      prob  \\\n",
       "0    round13  round13_Sample_381  round13_Sample_381_01   0     0  0.002097   \n",
       "1    round13  round13_Sample_381  round13_Sample_381_02   0     0  0.000535   \n",
       "2    round13  round13_Sample_381  round13_Sample_381_03   0     0  0.000505   \n",
       "3    round13  round13_Sample_381  round13_Sample_381_04   0     0  0.000516   \n",
       "4    round13  round13_Sample_381  round13_Sample_381_05   0     0  0.000485   \n",
       "..       ...                 ...                    ...  ..   ...       ...   \n",
       "897  round03   round03_Sample_29   round03_Sample_29_16   0     0  0.000730   \n",
       "898  round03   round03_Sample_29   round03_Sample_29_17   0     0  0.002010   \n",
       "899  round03   round03_Sample_29   round03_Sample_29_18   0     0  0.000725   \n",
       "900  round03   round03_Sample_29   round03_Sample_29_19   0     0  0.000761   \n",
       "901  round03   round03_Sample_29   round03_Sample_29_20   0     0  0.017674   \n",
       "\n",
       "        logit  best_pred  best_predfromprob  \n",
       "0   -6.165163          0                  0  \n",
       "1   -7.533374          0                  0  \n",
       "2   -7.591379          0                  0  \n",
       "3   -7.568815          0                  0  \n",
       "4   -7.631122          0                  0  \n",
       "..        ...        ...                ...  \n",
       "897 -7.221320          0                  0  \n",
       "898 -6.207634          0                  0  \n",
       "899 -7.228040          0                  0  \n",
       "900 -7.180627          0                  0  \n",
       "901 -4.017853          0                  0  \n",
       "\n",
       "[5221 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LF_results_dir = \"/home/NETID/gramacha/projects/report_matcher/qanda/baseline/LFBERT_crossvalidation/latestWOdev60_correct/thresholdtune/predictions/\"\n",
    "\n",
    "# LF_result_files = []\n",
    "# LF_full_preds = pd.DataFrame()\n",
    "\n",
    "\n",
    "# best_threshold = 0.05\n",
    "\n",
    "# def label_from_prob(val):\n",
    "#     if val >= best_threshold:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "    \n",
    "# for fold_num in foldwise_results:\n",
    "#     fold_results_file = f\"{LF_results_dir}fold{str(fold_num)}_predictions.csv\"\n",
    "#     LF_result_files.append(fold_results_file)\n",
    "#     fold_results = pd.read_csv(fold_results_file).drop(columns=[\"Unnamed: 0\"])\n",
    "#     LF_full_preds = pd.concat([LF_full_preds, fold_results])\n",
    "\n",
    "\n",
    "# predictions = (LF_full_preds['prob'].values >= best_threshold).astype(int).tolist()\n",
    "# LF_full_preds['best_pred'] = predictions\n",
    "\n",
    "# LF_full_preds['best_predfromprob'] = LF_full_preds['prob'].apply(label_from_prob)\n",
    " \n",
    "# # LF_full_preds[\"pred\"] = LF_full_preds[\"pred\"].astype('int')\n",
    "# LF_full_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LF_full_preds['best_pred'].sum()\n",
    "# LF_full_preds['best_predfromprob'].sum()\n",
    "# (LF_full_preds['best_pred'] - LF_full_preds['best_predfromprob']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LF_results_dir = \"/home/NETID/gramacha/projects/report_matcher/qanda/baseline/LFBERT_crossvalidation/latestWOdev60_correct/thresholdtune/predictions/thresh_0.05_preds.csv\"\n",
    "lf_res = pd.read_csv(LF_results_dir)\n",
    "predictions = [x for x in lf_res['pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in fulldatapreds.iterrows():\n",
    "    fulldatapreds.loc[idx, 'LongFormer_pred'] = lf_res.loc[lf_res['cand_key']==row['cand_key'], \"pred\"].values[0]\n",
    "fulldatapreds.to_csv(\"/home/NETID/gramacha/projects/report_matcher/qanda/sig test/full_data_preds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate if the F1 matches\n",
    "import sys\n",
    "sys.path.append(\"/home/NETID/gramacha/projects/report_matcher/report_matcher_src_code/src/\")\n",
    "import helperfactory.patientlevel_metrics as pat\n",
    "\n",
    "scores, pat_scores_msg, tpnpcounter = pat.get_patient_level_metrics(fulldatapreds, gt_colname='label', pred_colname='LongFormer_pred', \n",
    "                                                                    results_dir=\"\", resultsfilename=\"\")\n",
    "print(pat_scores_msg)\n",
    "print(scores[\"F1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concatenate LLaMa-3 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpt_prompt = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5221"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fulldatapreds = pd.read_csv(\"/home/NETID/gramacha/projects/report_matcher/qanda/sig test/full_data_preds.csv\")\n",
    "llama_res = pd.DataFrame()\n",
    "for i in range(5):\n",
    "    fold_res = pd.read_csv(f\"/home/NETID/gramacha/alignment-handbook/custom_dataset/gpt_adv_prompt/test/fold_{i}/preds.csv\")\n",
    "    llama_res = pd.concat([llama_res, fold_res])\n",
    "\n",
    "len(llama_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(text):\n",
    "    if use_gpt_prompt:\n",
    "        if text == '<answer start> False <answer end>':\n",
    "            return 0\n",
    "        if text == '<answer start> True <answer end>':\n",
    "            return 1\n",
    "        else:\n",
    "            print(f\"ERROR: {text}\")\n",
    "    else:\n",
    "        if text == '<answer start> NO <answer end>':\n",
    "            return 0\n",
    "        if text == '<answer start> YES <answer end>':\n",
    "            return 1\n",
    "        else:\n",
    "            print(f\"ERROR: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_res['pred'] = llama_res['pred'].apply(get_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fulldatapreds.rename(columns={'llama3':'llama3_gptbaseprompt'}, inplace=True)\n",
    "# fulldatapreds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in fulldatapreds.iterrows():\n",
    "    fulldatapreds.loc[idx, 'llama3_gptadvprompt'] = llama_res.loc[llama_res['cand_ids']==row['cand_key'], \"pred\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldatapreds.to_csv(\"/home/NETID/gramacha/projects/report_matcher/qanda/sig test/full_data_preds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.insert(0, \"/home/NETID/gramacha/projects/report_matcher/qanda/helperfactory/\")\n",
    "# import patientlevel_metrics\n",
    "\n",
    "# scores, results_msgbuf, patient_results_df = patientlevel_metrics.get_patient_level_metrics(lf_res, \"\", \"\", resultsfilename=\"\")\n",
    "# scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the classification report for all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.74      0.85      5389\n",
      "           1       0.21      0.86      0.33       418\n",
      "\n",
      "    accuracy                           0.75      5807\n",
      "   macro avg       0.60      0.80      0.59      5807\n",
      "weighted avg       0.93      0.75      0.81      5807\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      5389\n",
      "           1       0.81      0.81      0.81       418\n",
      "\n",
      "    accuracy                           0.97      5807\n",
      "   macro avg       0.90      0.90      0.90      5807\n",
      "weighted avg       0.97      0.97      0.97      5807\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95      5389\n",
      "           1       0.43      0.58      0.49       418\n",
      "\n",
      "    accuracy                           0.92      5807\n",
      "   macro avg       0.70      0.76      0.72      5807\n",
      "weighted avg       0.93      0.92      0.92      5807\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96      5389\n",
      "           1       0.49      0.30      0.37       418\n",
      "\n",
      "    accuracy                           0.93      5807\n",
      "   macro avg       0.72      0.64      0.67      5807\n",
      "weighted avg       0.91      0.93      0.92      5807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "predsdf = pd.read_csv(\"D:/projects/reportmatcher_results/report_matcher/error analysis/full_data_preds_wgpt.csv\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(classification_report(predsdf[\"label\"], predsdf[\"SVM_preds\"]))\n",
    "print(classification_report(predsdf[\"label\"], predsdf[\"gpt_preds\"]))\n",
    "print(classification_report(predsdf[\"label\"], predsdf[\"LF_preds\"]))\n",
    "print(classification_report(predsdf[\"label\"], predsdf[\"LLaMa_preds\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "integrate the recommendation annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpreds = pd.read_csv(\"/home/NETID/gramacha/projects/report_matcher/qanda/crossval-error analysis/full_data_preds_wgpt.csv\")\n",
    "\n",
    "for idx, row in allpreds.iterrows():\n",
    "    index = row[\"index_key\"]\n",
    "    anntext = \"\"\n",
    "    for split in [\"Train\", \"Dev\", \"Test\"]:\n",
    "        try:\n",
    "            with open(f\"/home/NETID/gramacha/data/Report_matcher/Split_Dataset2/{split}/Index/{index}.ann\", \"r\") as f:\n",
    "                anntext = f.read()\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    allpreds.loc[idx, \"recann\"] = anntext\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpreds.to_csv(\"/home/NETID/gramacha/projects/report_matcher/qanda/crossval-error analysis/full_data_preds_wann.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
